1) A Review of Speaker Diarization: Recent Advances with Deep Learning

Full Citation + Link

Park, T. J., Kanda, N., Dimitriadis, D., Han, K. J., Watanabe, S., & Narayanan, S. (2021). A Review of Speaker Diarization: Recent Advances with Deep Learning. Computer Speech & Language. Preprint available on arXiv: https://arxiv.org/abs/2101.09624
 

2101.09624v4

.

4–6 Sentence Summary

The paper reviews the development of speaker diarization—the task of determining “who spoke when”—from early modular methods to modern deep learning approaches. Traditional systems relied on segmentation, clustering, and embeddings such as i-vectors, but these suffered from limitations in noisy or overlapping speech. With the rise of deep learning, neural embeddings like d-vectors and x-vectors improved robustness, while end-to-end neural diarization (EEND) models promise joint optimization of modules for superior performance. The paper categorizes methods by their objectives (diarization vs. non-diarization) and optimization (single-module vs. joint), offering a taxonomy of approaches. It also discusses evaluation metrics like DER, JER, and WDER, and highlights challenges such as overlapping speech and integration with automatic speech recognition (ASR). Ultimately, the survey consolidates recent neural methods and identifies future opportunities for joint modeling and large-scale deployment.

3 Insights Learned

Taxonomy clarity: The field can be mapped into four categories—single-module optimization with or without diarization objectives, and joint optimization with or without diarization objectives—which helps compare diverse approaches systematically.

Neural embeddings shifted the game: Moving from i-vectors to x-vectors enabled better generalization across speakers and environments, reducing reliance on handcrafted features.

End-to-end models are promising but immature: EEND frameworks can directly handle overlapping speech, a long-standing weakness of traditional systems, but require massive annotated data and remain an open challenge for real-world scaling.

2 Limitations/Risks

Data dependency: Neural diarization methods often demand large-scale, labeled datasets, which are costly and difficult to acquire, especially for conversational or domain-specific speech.

Overlapping speech & scalability: While EEND models show promise, they struggle with variable speaker counts and online processing, limiting adoption in dynamic or large-scale applications.

1 Concrete Idea for Your Project

If your project involves processing multiparty audio/video (e.g., interviews, focus groups, design charrettes, or user workshops), you could integrate an EEND-based diarization system with downstream transcription/analytics tools. This would allow you to not only transcribe discussions but also attribute ideas and comments to individuals automatically, creating structured, speaker-linked datasets for analysis of participation patterns, collaboration, or stakeholder engagement.

2) What’s Under the Hood: Investigating Automatic Metrics on Meeting Summarization — Kirstein et al., Findings of EMNLP 2024. ACL Anthology page; PDF; arXiv. 
ACL Anthology

Full Citation + Link

Kirstein, F., Wahle, J. P., Ruas, T., & Gipp, B. (2024). What’s under the hood: Investigating Automatic Metrics on Meeting Summarization. arXiv preprint arXiv:2404.11124.
https://arxiv.org/abs/2404.11124

⸻

4–6 Sentence Summary

This paper addresses the problem of evaluating meeting summarization systems using automatic metrics that were not designed to capture the unique challenges of meeting data (e.g., speaker dynamics, contextual turn-taking). The authors review literature to establish a taxonomy of challenges and errors, then analyze summaries generated by encoder-decoder and decoder-only transformer models on the QMSum dataset. Human annotators marked errors such as missing information, hallucination, redundancy, and incoherence. The study compares these annotations with the performance of nine commonly used automatic metrics (ROUGE, BLEU, METEOR, BERTScore, etc.). Findings show that many metrics either fail to capture important errors or even reward flawed summaries, while only a few metrics correlate with human judgment in specific cases. The authors conclude that existing metrics are insufficient and suggest composite or LLM-based evaluation methods as a more promising future direction.

⸻

3 Insights Learned
	1.	A third of the tested metric–error correlations actually mask errors, meaning some metrics reward summaries that contain hallucinations or incorrect reasoning.
	2.	Encoder-decoder and decoder-only models produce distinct error profiles—encoder-decoders often struggle with coherence, while decoder-only LLMs are more prone to redundancies and wrong references.
	3.	Despite being widely used, ROUGE consistently underestimates error severity and fails to distinguish certain important quality dimensions, reinforcing its limitations in evaluating abstractive meeting summaries.

⸻

2 Limitations/Risks
	1.	The study focuses only on English-language meeting data (QMSum), which may limit the generalizability of findings to other languages and meeting formats.
	2.	The reliance on a relatively small annotated sample (175 generated summaries) and a narrow set of models may not capture the full spectrum of errors in real-world meeting summarization systems.

⸻

1 Concrete Idea for Your Project

Since your project deals with autogeneration and assignment of user stories in Agile Software Development (ASD), one idea is to adapt the paper’s challenge–error taxonomy to your domain. For instance, instead of meeting-specific errors (e.g., wrong references), you could define ASD-specific ones (e.g., missing acceptance criteria, ambiguous requirements). Then, you can design or fine-tune automatic evaluation metrics that capture these domain-specific quality issues, ensuring that generated user stories are not just fluent but also actionable and correctly assigned to the right user.


3) Automated User Story Generation with Test Case Specification Using Large Language Model

Full Citation + Link

Rahman, T., & Zhu, Y. (2024). Automated User Story Generation with Test Case Specification Using Large Language Model. arXiv preprint arXiv:2404.01558. https://arxiv.org/abs/2404.01558
 

2404.01558v1

.

4–6 Sentence Summary

This paper addresses the challenge of automating requirements engineering (RE) in Agile software development, specifically generating user stories and test cases from requirements documents. Traditionally, this process consumes significant time and effort from senior engineers and project managers. The authors propose GeneUS, a tool built on GPT-4, which applies a novel prompting technique called Refine and Thought (RaT) to filter redundant tokens and reduce hallucinations. GeneUS produces structured JSON outputs with user stories, definitions of done, functional/non-functional constraints, and test specifications, making them directly integrable with project management systems like Jira. Evaluation was conducted using seven requirements documents and a survey (RUST framework) of 50 developers across industry and academia. Results showed high acceptance (median score 4/5), though areas like specifiability and technical detail require improvement.

3 Insights Learned

Prompting matters greatly: The RaT technique (a variant of Chain of Thought prompting) significantly reduces hallucinations by refining input before reasoning, which is crucial for handling long and noisy requirements documents.

Bridging RE and automation: By producing JSON outputs with structured deliverables and test coverage, GeneUS creates a smooth path for downstream integration into Agile tools, moving closer to “AutoAgile”—end-to-end automated software engineering.

Developer validation is key: The RUST survey revealed that practitioners found the outputs mostly useful, especially for readability and understandability, but flagged weaknesses in detail and technical alignment—highlighting that automation can accelerate, but not yet replace, human oversight.

2 Limitations/Risks

Hallucination risk: Despite RaT, large language models still sometimes generate incomplete or fabricated technical details, which could mislead developers if unchecked.

Context/data limitations: The tool currently works only with text-based requirements and lacks support for diagrams, mockups, or multimodal inputs—limiting applicability in real-world RE documents.

1 Concrete Idea for Your Project

If your project involves design workflows or documentation automation (e.g., turning architectural briefs or case study requirements into structured tasks), you could adapt the GeneUS approach: feed project briefs into an LLM with RaT-style prompting to auto-generate design tasks + acceptance criteria + test checks in JSON. This could serve as a foundation for structured design reviews or linking tasks directly into Trello/Jira for tracking architectural project progress.


4) Building Real-World Meeting Summarization Systems Using LLMs: A Practical Perspective — Laskar et al., EMNLP Industry 2023.

Full Citation + Link

Laskar, M. T. R., Fu, X.-Y., Chen, C., & Bhushan, S. T. N. (2023). Building Real-World Meeting Summarization Systems using Large Language Models: A Practical Perspective. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track (pp. 343–352). Association for Computational Linguistics.
https://doi.org/10.18653/v1/2023.emnlp-industry.33

⸻

4–6 Sentence Summary

This paper tackles the challenge of building meeting summarization systems suitable for real-world industrial deployment using large language models (LLMs). The authors conduct extensive experiments comparing closed-source models (GPT-3.5, GPT-4, PaLM-2) and open-source models (LLaMA-2 7B, 13B) across multiple meeting datasets (QMSUM, AMI, ICSI). They introduce two strategies to handle the long transcript problem: summarization via truncation and summarization via chapterization (splitting transcripts into segments and summarizing them hierarchically). Results show that while closed-source LLMs like GPT-4 achieve the highest accuracy, smaller open-source models like LLaMA-2 can deliver competitive performance at a fraction of the cost and with fewer privacy risks. The authors ultimately deploy LLaMA-2-7B in production, highlighting the balance between performance, cost, and data privacy.

⸻

3 Insights Learned
	1.	Chapterization vs. Truncation trade-off: Surprisingly, truncation sometimes outperformed chapterization in datasets where summaries were biased toward the beginning of transcripts.
	2.	Cost-performance gap: GPT-4 is 25× more expensive than GPT-3.5, but not proportionally better in results, making smaller models attractive for industrial adoption.
	3.	Privacy and reliability matter: Closed-source APIs not only raise privacy concerns (sending sensitive meeting data externally) but also risk API slowdowns/failures, which heavily impact real-world usability.

⸻

2 Limitations/Risks
	1.	The study evaluates only on academic datasets rather than fully proprietary business meeting data, which may reduce ecological validity.
	2.	The evaluation relies on automatic metrics (ROUGE, BERTScore), which do not always reflect real user satisfaction, and lacks comprehensive human evaluation.

⸻

1 Concrete Idea for Your Project

For your project on autogeneration and assignment of user stories in Agile Software Development (ASD), you could apply the paper’s chapterization strategy. Instead of meeting transcripts, break down long product requirement discussions into segments, generate partial user stories, and then synthesize them into complete, coherent stories. This hierarchical generation method would ensure coverage of all requirements while making the final output more structured and assignable to team members.








