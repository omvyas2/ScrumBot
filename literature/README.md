Citations - BibTex


1. Automated User Story Generation with Test Case Specification Using Large Language Model

@misc{rahman2024automateduserstorygeneration,
      title={Automated User Story Generation with Test Case Specification Using Large Language Model}, 
      author={Tajmilur Rahman and Yuecai Zhu},
      year={2024},
      eprint={2404.01558},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2404.01558}, 
}

2. A Review of Speaker Diarization: Recent Advances with Deep Learning

@misc{park2021reviewspeakerdiarizationrecent,
      title={A Review of Speaker Diarization: Recent Advances with Deep Learning}, 
      author={Tae Jin Park and Naoyuki Kanda and Dimitrios Dimitriadis and Kyu J. Han and Shinji Watanabe and Shrikanth Narayanan},
      year={2021},
      eprint={2101.09624},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2101.09624}, 
}

3. Building Real-World Meeting Summarization Systems Using LLMs: A Practical Perspective 

@misc{laskar2023buildingrealworldmeetingsummarization,
      title={Building Real-World Meeting Summarization Systems using Large Language Models: A Practical Perspective}, 
      author={Md Tahmid Rahman Laskar and Xue-Yong Fu and Cheng Chen and Shashi Bhushan TN},
      year={2023},
      eprint={2310.19233},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.19233}, 
}

4. What’s Under the Hood: Investigating Automatic Metrics on Meeting Summarization

@inproceedings{Kirstein_2024,
   title={What’s under the hood: Investigating Automatic Metrics on Meeting Summarization},
   url={http://dx.doi.org/10.18653/v1/2024.findings-emnlp.393},
   DOI={10.18653/v1/2024.findings-emnlp.393},
   booktitle={Findings of the Association for Computational Linguistics: EMNLP 2024},
   publisher={Association for Computational Linguistics},
   author={Kirstein, Frederic and Wahle, Jan Philip and Ruas, Terry and Gipp, Bela},
   year={2024},
   pages={6709–6723} }

5. ExplainMeetSum: A Dataset for Explainable Meeting Summarization Aligned with Human Intent
   
@inproceedings{kim-etal-2023-explainmeetsum,
    title = "{E}xplain{M}eet{S}um: A Dataset for Explainable Meeting Summarization Aligned with Human Intent",
    author = "Kim, Hyun  and
      Cho, Minsoo  and
      Na, Seung-Hoon",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.731/",
    doi = "10.18653/v1/2023.acl-long.731",
    pages = "13079--13098",
    abstract = "To enhance the explainability of meeting summarization, we construct a new dataset called ``ExplainMeetSum,'' an augmented version of QMSum, by newly annotating evidence sentences that faithfully ``explain'' a summary. Using ExplainMeetSum, we propose a novel multiple extractor guided summarization, namely Multi-DYLE, which extensively generalizes DYLE to enable using a supervised extractor based on human-aligned extractive oracles. We further present an explainability-aware task, named ``Explainable Evidence Extraction'' (E3), which aims to automatically detect all evidence sentences that support a given summary. Experimental results on the QMSum dataset show that the proposed Multi-DYLE outperforms DYLE with gains of up to 3.13 in the ROUGE-1 score. We further present the initial results on the E3 task, under the settings using separate and joint evaluation metrics."
}

6. Action-Item-Driven Summarization of Long Meeting Transcripts

   @misc{golia2024actionitemdrivensummarizationlongmeeting,
      title={Action-Item-Driven Summarization of Long Meeting Transcripts}, 
      author={Logan Golia and Jugal Kalita},
      year={2024},
      eprint={2312.17581},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.17581}, 
}

7. Who should fix this bug?

   @inproceedings{10.1145/1134285.1134336,
author = {Anvik, John and Hiew, Lyndon and Murphy, Gail C.},
title = {Who should fix this bug?},
year = {2006},
isbn = {1595933751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1134285.1134336},
doi = {10.1145/1134285.1134336},
abstract = {Open source development projects typically support an open bug repository to which both developers and users can report bugs. The reports that appear in this repository must be triaged to determine if the report is one which requires attention and if it is, which developer will be assigned the responsibility of resolving the report. Large open source developments are burdened by the rate at which new bug reports appear in the bug repository. In this paper, we present a semi-automated approach intended to ease one part of this process, the assignment of reports to a developer. Our approach applies a machine learning algorithm to the open bug repository to learn the kinds of reports each developer resolves. When a new report arrives, the classifier produced by the machine learning technique suggests a small number of developers suitable to resolve the report. With this approach, we have reached precision levels of 57\% and 64\% on the Eclipse and Firefox development projects respectively. We have also applied our approach to the gcc open source development with less positive results. We describe the conditions under which the approach is applicable and also report on the lessons we learned about applying machine learning to repositories used in open source development.},
booktitle = {Proceedings of the 28th International Conference on Software Engineering},
pages = {361–370},
numpages = {10},
keywords = {bug report assignment, bug triage, issue tracking, machine learning, problem tracking},
location = {Shanghai, China},
series = {ICSE '06}
}

   


